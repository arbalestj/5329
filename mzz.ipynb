{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Reading images from cifar10_data/cifar-10-batches-py/data_batch_1\n",
      "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Reading images from cifar10_data/cifar-10-batches-py/data_batch_2\n",
      "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Reading images from cifar10_data/cifar-10-batches-py/data_batch_3\n",
      "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Reading images from cifar10_data/cifar-10-batches-py/data_batch_4\n",
      "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Reading images from cifar10_data/cifar-10-batches-py/data_batch_5\n",
      "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Shuffling\n",
      "Reading images from cifar10_data/cifar-10-batches-py/test_batch\n",
      "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Shuffling\n",
      "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Administrator\\5329ass2\\resnet-in-tensorflow-master\\resnet.py:49: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From <ipython-input-4-5ae94764f107>:291: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Start training...\n",
      "----------------------------\n",
      "2019-05-23 00:11:26.885606: step 0, loss = 2.4880 (24.8 examples/sec; 5.167 sec/batch)\n",
      "Train top1 error =  0.890625\n",
      "Validation top1 error = 0.8840\n",
      "Validation loss =  2.3723366\n",
      "----------------------------\n",
      "2019-05-23 00:11:50.076576: step 10, loss = 2.1945 (80.6 examples/sec; 1.589 sec/batch)\n",
      "Train top1 error =  0.765625\n",
      "Validation top1 error = 0.7960\n",
      "Validation loss =  2.055035\n",
      "----------------------------\n",
      "2019-05-23 00:12:08.284934: step 20, loss = 2.1238 (81.7 examples/sec; 1.567 sec/batch)\n",
      "Train top1 error =  0.765625\n",
      "Validation top1 error = 0.7160\n",
      "Validation loss =  1.9292572\n",
      "----------------------------\n",
      "2019-05-23 00:12:26.412376: step 30, loss = 1.9488 (79.2 examples/sec; 1.615 sec/batch)\n",
      "Train top1 error =  0.7421875\n",
      "Validation top1 error = 0.6880\n",
      "Validation loss =  1.8257277\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Coder: Wenxin Xu\n",
    "# Github: https://github.com/wenxinxu/resnet_in_tensorflow\n",
    "# ==============================================================================\n",
    "\n",
    "from resnet import *\n",
    "from datetime import datetime\n",
    "import time\n",
    "from cifar10_input import *\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "class Train(object):\n",
    "    '''\n",
    "    This Object is responsible for all the training and validation process\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # Set up all the placeholders\n",
    "        self.placeholders()\n",
    "\n",
    "\n",
    "    def placeholders(self):\n",
    "        '''\n",
    "        There are five placeholders in total.\n",
    "        image_placeholder and label_placeholder are for train images and labels\n",
    "        vali_image_placeholder and vali_label_placeholder are for validation imgaes and labels\n",
    "        lr_placeholder is for learning rate. Feed in learning rate each time of training\n",
    "        implements learning rate decay easily\n",
    "        '''\n",
    "        self.image_placeholder = tf.placeholder(dtype=tf.float32,\n",
    "                                                shape=[FLAGS.train_batch_size, IMG_HEIGHT,\n",
    "                                                        IMG_WIDTH, IMG_DEPTH])\n",
    "        self.label_placeholder = tf.placeholder(dtype=tf.int32, shape=[FLAGS.train_batch_size])\n",
    "\n",
    "        self.vali_image_placeholder = tf.placeholder(dtype=tf.float32, shape=[FLAGS.validation_batch_size,\n",
    "                                                                IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH])\n",
    "        self.vali_label_placeholder = tf.placeholder(dtype=tf.int32, shape=[FLAGS.validation_batch_size])\n",
    "\n",
    "        self.lr_placeholder = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "\n",
    "\n",
    "\n",
    "    def build_train_validation_graph(self):\n",
    "        '''\n",
    "        This function builds the train graph and validation graph at the same time.\n",
    "        \n",
    "        '''\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        validation_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        # Logits of training data and valiation data come from the same graph. The inference of\n",
    "        # validation data share all the weights with train data. This is implemented by passing\n",
    "        # reuse=True to the variable scopes of train graph\n",
    "        logits = inference(self.image_placeholder, FLAGS.num_residual_blocks, reuse=False)\n",
    "        vali_logits = inference(self.vali_image_placeholder, FLAGS.num_residual_blocks, reuse=True)\n",
    "\n",
    "        # The following codes calculate the train loss, which is consist of the\n",
    "        # softmax cross entropy and the relularization loss\n",
    "        regu_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        loss = self.loss(logits, self.label_placeholder)\n",
    "        self.full_loss = tf.add_n([loss] + regu_losses)\n",
    "\n",
    "        predictions = tf.nn.softmax(logits)\n",
    "        self.train_top1_error = self.top_k_error(predictions, self.label_placeholder, 1)\n",
    "\n",
    "\n",
    "        # Validation loss\n",
    "        self.vali_loss = self.loss(vali_logits, self.vali_label_placeholder)\n",
    "        vali_predictions = tf.nn.softmax(vali_logits)\n",
    "        self.vali_top1_error = self.top_k_error(vali_predictions, self.vali_label_placeholder, 1)\n",
    "\n",
    "        self.train_op, self.train_ema_op = self.train_operation(global_step, self.full_loss,\n",
    "                                                                self.train_top1_error)\n",
    "        self.val_op = self.validation_op(validation_step, self.vali_top1_error, self.vali_loss)\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        '''\n",
    "        This is the main function for training\n",
    "        '''\n",
    "\n",
    "        # For the first step, we are loading all training images and validation images into the\n",
    "        # memory\n",
    "        all_data, all_labels = prepare_train_data(padding_size=FLAGS.padding_size)\n",
    "        vali_data, vali_labels = read_validation_data()\n",
    "\n",
    "        # Build the graph for train and validation\n",
    "        self.build_train_validation_graph()\n",
    "\n",
    "        # Initialize a saver to save checkpoints. Merge all summaries, so we can run all\n",
    "        # summarizing operations by running summary_op. Initialize a new session\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        init = tf.initialize_all_variables()\n",
    "        sess = tf.Session()\n",
    "\n",
    "\n",
    "        # If you want to load from a checkpoint\n",
    "        if FLAGS.is_use_ckpt is True:\n",
    "            saver.restore(sess, FLAGS.ckpt_path)\n",
    "            print('Restored from checkpoint...')\n",
    "        else:\n",
    "            sess.run(init)\n",
    "\n",
    "        # This summary writer object helps write summaries on tensorboard\n",
    "        summary_writer = tf.summary.FileWriter(train_dir, sess.graph)\n",
    "\n",
    "\n",
    "        # These lists are used to save a csv file at last\n",
    "        step_list = []\n",
    "        train_error_list = []\n",
    "        val_error_list = []\n",
    "\n",
    "        print('Start training...')\n",
    "        print('----------------------------')\n",
    "\n",
    "        for step in range(FLAGS.train_steps):\n",
    "\n",
    "            train_batch_data, train_batch_labels = self.generate_augment_train_batch(all_data, all_labels,\n",
    "                                                                        FLAGS.train_batch_size)\n",
    "\n",
    "\n",
    "            validation_batch_data, validation_batch_labels = self.generate_vali_batch(vali_data,\n",
    "                                                           vali_labels, FLAGS.validation_batch_size)\n",
    "\n",
    "            # Want to validate once before training. You may check the theoretical validation\n",
    "            # loss first\n",
    "            if step % FLAGS.report_freq == 0:\n",
    "\n",
    "                if FLAGS.is_full_validation is True:\n",
    "                    validation_loss_value, validation_error_value = self.full_validation(loss=self.vali_loss,\n",
    "                                            top1_error=self.vali_top1_error, vali_data=vali_data,\n",
    "                                            vali_labels=vali_labels, session=sess,\n",
    "                                            batch_data=train_batch_data, batch_label=train_batch_labels)\n",
    "\n",
    "                    vali_summ = tf.Summary()\n",
    "                    vali_summ.value.add(tag='full_validation_error',\n",
    "                                        simple_value=validation_error_value.astype(np.float))\n",
    "                    summary_writer.add_summary(vali_summ, step)\n",
    "                    summary_writer.flush()\n",
    "\n",
    "                else:\n",
    "                    _, validation_error_value, validation_loss_value = sess.run([self.val_op,\n",
    "                                                                     self.vali_top1_error,\n",
    "                                                                 self.vali_loss],\n",
    "                                                {self.image_placeholder: train_batch_data,\n",
    "                                                 self.label_placeholder: train_batch_labels,\n",
    "                                                 self.vali_image_placeholder: validation_batch_data,\n",
    "                                                 self.vali_label_placeholder: validation_batch_labels,\n",
    "                                                 self.lr_placeholder: FLAGS.init_lr})\n",
    "\n",
    "                val_error_list.append(validation_error_value)\n",
    "\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            _, _, train_loss_value, train_error_value = sess.run([self.train_op, self.train_ema_op,\n",
    "                                                           self.full_loss, self.train_top1_error],\n",
    "                                {self.image_placeholder: train_batch_data,\n",
    "                                  self.label_placeholder: train_batch_labels,\n",
    "                                  self.vali_image_placeholder: validation_batch_data,\n",
    "                                  self.vali_label_placeholder: validation_batch_labels,\n",
    "                                  self.lr_placeholder: FLAGS.init_lr})\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "\n",
    "            if step % FLAGS.report_freq == 0:\n",
    "                summary_str = sess.run(summary_op, {self.image_placeholder: train_batch_data,\n",
    "                                                    self.label_placeholder: train_batch_labels,\n",
    "                                                    self.vali_image_placeholder: validation_batch_data,\n",
    "                                                    self.vali_label_placeholder: validation_batch_labels,\n",
    "                                                    self.lr_placeholder: FLAGS.init_lr})\n",
    "                summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "                num_examples_per_step = FLAGS.train_batch_size\n",
    "                examples_per_sec = num_examples_per_step / duration\n",
    "                sec_per_batch = float(duration)\n",
    "\n",
    "                format_str = ('%s: step %d, loss = %.4f (%.1f examples/sec; %.3f ' 'sec/batch)')\n",
    "                print(format_str % (datetime.now(), step, train_loss_value, examples_per_sec,\n",
    "                                    sec_per_batch))\n",
    "                print('Train top1 error = ', train_error_value)\n",
    "                print('Validation top1 error = %.4f' % validation_error_value)\n",
    "                print('Validation loss = ', validation_loss_value)\n",
    "                print('----------------------------')\n",
    "\n",
    "                step_list.append(step)\n",
    "                train_error_list.append(train_error_value)\n",
    "\n",
    "\n",
    "\n",
    "            if step == FLAGS.decay_step0 or step == FLAGS.decay_step1:\n",
    "                FLAGS.init_lr = 0.1 * FLAGS.init_lr\n",
    "                print('Learning rate decayed to ', FLAGS.init_lr)\n",
    "\n",
    "            # Save checkpoints every 10000 steps\n",
    "            if step % 10000 == 0 or (step + 1) == FLAGS.train_steps:\n",
    "                checkpoint_path = os.path.join(train_dir, 'model.ckpt')\n",
    "                saver.save(sess, checkpoint_path, global_step=step)\n",
    "\n",
    "                df = pd.DataFrame(data={'step':step_list, 'train_error':train_error_list,\n",
    "                                'validation_error': val_error_list})\n",
    "                df.to_csv(train_dir + FLAGS.version + '_error.csv')\n",
    "\n",
    "\n",
    "    def test(self, test_image_array):\n",
    "        '''\n",
    "        This function is used to evaluate the test data. Please finish pre-precessing in advance\n",
    "\n",
    "        :param test_image_array: 4D numpy array with shape [num_test_images, img_height, img_width,\n",
    "        img_depth]\n",
    "        :return: the softmax probability with shape [num_test_images, num_labels]\n",
    "        '''\n",
    "        num_test_images = len(test_image_array)\n",
    "        num_batches = num_test_images // FLAGS.test_batch_size\n",
    "        remain_images = num_test_images % FLAGS.test_batch_size\n",
    "        print('%i test batches in total...' %num_batches)\n",
    "\n",
    "        # Create the test image and labels placeholders\n",
    "        self.test_image_placeholder = tf.placeholder(dtype=tf.float32, shape=[FLAGS.test_batch_size,\n",
    "                                                        IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH])\n",
    "\n",
    "        # Build the test graph\n",
    "        logits = inference(self.test_image_placeholder, FLAGS.num_residual_blocks, reuse=False)\n",
    "        predictions = tf.nn.softmax(logits)\n",
    "\n",
    "        # Initialize a new session and restore a checkpoint\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        sess = tf.Session()\n",
    "\n",
    "        saver.restore(sess, FLAGS.test_ckpt_path)\n",
    "        print('Model restored from ', FLAGS.test_ckpt_path)\n",
    "\n",
    "        prediction_array = np.array([]).reshape(-1, NUM_CLASS)\n",
    "        # Test by batches\n",
    "        for step in range(num_batches):\n",
    "            if step % 10 == 0:\n",
    "                print('%i batches finished!' %step)\n",
    "            offset = step * FLAGS.test_batch_size\n",
    "            test_image_batch = test_image_array[offset:offset+FLAGS.test_batch_size, ...]\n",
    "\n",
    "            batch_prediction_array = sess.run(predictions,\n",
    "                                        feed_dict={self.test_image_placeholder: test_image_batch})\n",
    "\n",
    "            prediction_array = np.concatenate((prediction_array, batch_prediction_array))\n",
    "\n",
    "        # If test_batch_size is not a divisor of num_test_images\n",
    "        if remain_images != 0:\n",
    "            self.test_image_placeholder = tf.placeholder(dtype=tf.float32, shape=[remain_images,\n",
    "                                                        IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH])\n",
    "            # Build the test graph\n",
    "            logits = inference(self.test_image_placeholder, FLAGS.num_residual_blocks, reuse=True)\n",
    "            predictions = tf.nn.softmax(logits)\n",
    "\n",
    "            test_image_batch = test_image_array[-remain_images:, ...]\n",
    "\n",
    "            batch_prediction_array = sess.run(predictions, feed_dict={\n",
    "                self.test_image_placeholder: test_image_batch})\n",
    "\n",
    "            prediction_array = np.concatenate((prediction_array, batch_prediction_array))\n",
    "\n",
    "        return prediction_array\n",
    "\n",
    "\n",
    "\n",
    "    ## Helper functions\n",
    "    def loss(self, logits, labels):\n",
    "        '''\n",
    "        Calculate the cross entropy loss given logits and true labels\n",
    "        :param logits: 2D tensor with shape [batch_size, num_labels]\n",
    "        :param labels: 1D tensor with shape [batch_size]\n",
    "        :return: loss tensor with shape [1]\n",
    "        '''\n",
    "        labels = tf.cast(labels, tf.int64)\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                                       labels=labels, name='cross_entropy_per_example')\n",
    "        cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "        return cross_entropy_mean\n",
    "\n",
    "\n",
    "    def top_k_error(self, predictions, labels, k):\n",
    "        '''\n",
    "        Calculate the top-k error\n",
    "        :param predictions: 2D tensor with shape [batch_size, num_labels]\n",
    "        :param labels: 1D tensor with shape [batch_size, 1]\n",
    "        :param k: int\n",
    "        :return: tensor with shape [1]\n",
    "        '''\n",
    "        batch_size = predictions.get_shape().as_list()[0]\n",
    "        in_top1 = tf.to_float(tf.nn.in_top_k(predictions, labels, k=1))\n",
    "        num_correct = tf.reduce_sum(in_top1)\n",
    "        return (batch_size - num_correct) / float(batch_size)\n",
    "\n",
    "\n",
    "    def generate_vali_batch(self, vali_data, vali_label, vali_batch_size):\n",
    "        '''\n",
    "        If you want to use a random batch of validation data to validate instead of using the\n",
    "        whole validation data, this function helps you generate that batch\n",
    "        :param vali_data: 4D numpy array\n",
    "        :param vali_label: 1D numpy array\n",
    "        :param vali_batch_size: int\n",
    "        :return: 4D numpy array and 1D numpy array\n",
    "        '''\n",
    "        offset = np.random.choice(10000 - vali_batch_size, 1)[0]\n",
    "        vali_data_batch = vali_data[offset:offset+vali_batch_size, ...]\n",
    "        vali_label_batch = vali_label[offset:offset+vali_batch_size]\n",
    "        return vali_data_batch, vali_label_batch\n",
    "\n",
    "\n",
    "    def generate_augment_train_batch(self, train_data, train_labels, train_batch_size):\n",
    "        '''\n",
    "        This function helps generate a batch of train data, and random crop, horizontally flip\n",
    "        and whiten them at the same time\n",
    "        :param train_data: 4D numpy array\n",
    "        :param train_labels: 1D numpy array\n",
    "        :param train_batch_size: int\n",
    "        :return: augmented train batch data and labels. 4D numpy array and 1D numpy array\n",
    "        '''\n",
    "        offset = np.random.choice(EPOCH_SIZE - train_batch_size, 1)[0]\n",
    "        batch_data = train_data[offset:offset+train_batch_size, ...]\n",
    "        batch_data = random_crop_and_flip(batch_data, padding_size=FLAGS.padding_size)\n",
    "\n",
    "        batch_data = whitening_image(batch_data)\n",
    "        batch_label = train_labels[offset:offset+FLAGS.train_batch_size]\n",
    "\n",
    "        return batch_data, batch_label\n",
    "\n",
    "\n",
    "    def train_operation(self, global_step, total_loss, top1_error):\n",
    "        '''\n",
    "        Defines train operations\n",
    "        :param global_step: tensor variable with shape [1]\n",
    "        :param total_loss: tensor with shape [1]\n",
    "        :param top1_error: tensor with shape [1]\n",
    "        :return: two operations. Running train_op will do optimization once. Running train_ema_op\n",
    "        will generate the moving average of train error and train loss for tensorboard\n",
    "        '''\n",
    "        # Add train_loss, current learning rate and train error into the tensorboard summary ops\n",
    "        tf.summary.scalar('learning_rate', self.lr_placeholder)\n",
    "        tf.summary.scalar('train_loss', total_loss)\n",
    "        tf.summary.scalar('train_top1_error', top1_error)\n",
    "\n",
    "        # The ema object help calculate the moving average of train loss and train error\n",
    "        ema = tf.train.ExponentialMovingAverage(FLAGS.train_ema_decay, global_step)\n",
    "        train_ema_op = ema.apply([total_loss, top1_error])\n",
    "        tf.summary.scalar('train_top1_error_avg', ema.average(top1_error))\n",
    "        tf.summary.scalar('train_loss_avg', ema.average(total_loss))\n",
    "\n",
    "        opt = tf.train.MomentumOptimizer(learning_rate=self.lr_placeholder, momentum=0.9)\n",
    "        train_op = opt.minimize(total_loss, global_step=global_step)\n",
    "        return train_op, train_ema_op\n",
    "\n",
    "\n",
    "    def validation_op(self, validation_step, top1_error, loss):\n",
    "        '''\n",
    "        Defines validation operations\n",
    "        :param validation_step: tensor with shape [1]\n",
    "        :param top1_error: tensor with shape [1]\n",
    "        :param loss: tensor with shape [1]\n",
    "        :return: validation operation\n",
    "        '''\n",
    "\n",
    "        # This ema object help calculate the moving average of validation loss and error\n",
    "\n",
    "        # ema with decay = 0.0 won't average things at all. This returns the original error\n",
    "        ema = tf.train.ExponentialMovingAverage(0.0, validation_step)\n",
    "        ema2 = tf.train.ExponentialMovingAverage(0.95, validation_step)\n",
    "\n",
    "\n",
    "        val_op = tf.group(validation_step.assign_add(1), ema.apply([top1_error, loss]),\n",
    "                          ema2.apply([top1_error, loss]))\n",
    "        top1_error_val = ema.average(top1_error)\n",
    "        top1_error_avg = ema2.average(top1_error)\n",
    "        loss_val = ema.average(loss)\n",
    "        loss_val_avg = ema2.average(loss)\n",
    "\n",
    "        # Summarize these values on tensorboard\n",
    "        tf.summary.scalar('val_top1_error', top1_error_val)\n",
    "        tf.summary.scalar('val_top1_error_avg', top1_error_avg)\n",
    "        tf.summary.scalar('val_loss', loss_val)\n",
    "        tf.summary.scalar('val_loss_avg', loss_val_avg)\n",
    "        return val_op\n",
    "\n",
    "\n",
    "    def full_validation(self, loss, top1_error, session, vali_data, vali_labels, batch_data,\n",
    "                        batch_label):\n",
    "        '''\n",
    "        Runs validation on all the 10000 valdiation images\n",
    "        :param loss: tensor with shape [1]\n",
    "        :param top1_error: tensor with shape [1]\n",
    "        :param session: the current tensorflow session\n",
    "        :param vali_data: 4D numpy array\n",
    "        :param vali_labels: 1D numpy array\n",
    "        :param batch_data: 4D numpy array. training batch to feed dict and fetch the weights\n",
    "        :param batch_label: 1D numpy array. training labels to feed the dict\n",
    "        :return: float, float\n",
    "        '''\n",
    "        num_batches = 10000 // FLAGS.validation_batch_size\n",
    "        order = np.random.choice(10000, num_batches * FLAGS.validation_batch_size)\n",
    "        vali_data_subset = vali_data[order, ...]\n",
    "        vali_labels_subset = vali_labels[order]\n",
    "\n",
    "        loss_list = []\n",
    "        error_list = []\n",
    "\n",
    "        for step in range(num_batches):\n",
    "            offset = step * FLAGS.validation_batch_size\n",
    "            feed_dict = {self.image_placeholder: batch_data, self.label_placeholder: batch_label,\n",
    "                self.vali_image_placeholder: vali_data_subset[offset:offset+FLAGS.validation_batch_size, ...],\n",
    "                self.vali_label_placeholder: vali_labels_subset[offset:offset+FLAGS.validation_batch_size],\n",
    "                self.lr_placeholder: FLAGS.init_lr}\n",
    "            loss_value, top1_error_value = session.run([loss, top1_error], feed_dict=feed_dict)\n",
    "            loss_list.append(loss_value)\n",
    "            error_list.append(top1_error_value)\n",
    "\n",
    "        return np.mean(loss_list), np.mean(error_list)\n",
    "\n",
    "\n",
    "maybe_download_and_extract()\n",
    "# Initialize the Train object\n",
    "train = Train()\n",
    "# Start the training session\n",
    "train.train()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
